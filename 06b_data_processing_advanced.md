[‚¨ÖÔ∏è Back to Main SDLC Page](00_data_platform_sdlc.md) | [‚¨ÖÔ∏è Back to Data Processing Part 1](06_data_processing.md)

# Advanced Data Processing Topics
## Stream Processing, ML Pipelines & Performance Optimization

**Purpose:** This document covers advanced data processing topics including real-time stream processing, machine learning pipelines, advanced optimization strategies, and comprehensive real-world implementation scenarios.

---

## üåä Stream Processing Architecture

### 6.1 Apache Kafka Streams Implementation

#### Kafka Streams Processing Framework

```java
// KafkaStreamsProcessor.java
package com.dataplatform.streaming;

import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.Stores;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.time.Duration;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class KafkaStreamsProcessor {
    
    private static final String APPLICATION_ID = "data-processing-streams";
    private static final String BOOTSTRAP_SERVERS = "localhost:9092";
    private static final ObjectMapper objectMapper = new ObjectMapper();
    
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE_V2);
        
        StreamsBuilder builder = new StreamsBuilder();
        
        // Build processing topology
        buildProcessingTopology(builder);
        
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        CountDownLatch latch = new CountDownLatch(1);
        
        // Attach shutdown handler
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });
        
        try {
            streams.start();
            latch.await();
        } catch (Throwable e) {
            System.exit(1);
        }
        System.exit(0);
    }
    
    private static void buildProcessingTopology(StreamsBuilder builder) {
        
        // Input streams
        KStream<String, String> rawEvents = builder.stream("raw-events");
        KStream<String, String> userProfiles = builder.stream("user-profiles");
        
        // Create state store for user enrichment
        builder.addStateStore(Stores.keyValueStoreBuilder(
            Stores.persistentKeyValueStore("user-store"),
            Serdes.String(),
            Serdes.String()
        ));
        
        // Process raw events
        KStream<String, String> processedEvents = rawEvents
            .filter((key, value) -> isValidEvent(value))
            .mapValues(KafkaStreamsProcessor::enrichEvent)
            .transformValues(() -> new EventValidator(), "user-store");
        
        // Aggregate events by user and time window
        KTable<Windowed<String>, Long> eventCounts = processedEvents
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count();
        
        // Detect anomalies
        KStream<String, String> anomalies = eventCounts
            .toStream()
            .filter((windowedKey, count) -> count > 100) // Threshold for anomaly
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key(),
                createAnomalyAlert(windowedKey.key(), count, windowedKey.window())
            ));
        
        // Output streams
        processedEvents.to("processed-events");
        anomalies.to("anomaly-alerts");
        
        // Real-time aggregations
        processedEvents
            .groupBy((key, value) -> extractEventType(value))
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .count()
            .toStream()
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key(),
                createMetricEvent(windowedKey.key(), count, windowedKey.window())
            ))
            .to("real-time-metrics");
    }
    
    private static boolean isValidEvent(String eventJson) {
        try {
            JsonNode event = objectMapper.readTree(eventJson);
            return event.has("userId") && 
                   event.has("eventType") && 
                   event.has("timestamp");
        } catch (Exception e) {
            return false;
        }
    }
    
    private static String enrichEvent(String eventJson) {
        try {
            JsonNode event = objectMapper.readTree(eventJson);
            
            // Add processing metadata
            ((com.fasterxml.jackson.databind.node.ObjectNode) event)
                .put("processedAt", System.currentTimeMillis())
                .put("processingVersion", "1.0");
            
            return objectMapper.writeValueAsString(event);
        } catch (Exception e) {
            return eventJson;
        }
    }
    
    private static String extractEventType(String eventJson) {
        try {
            JsonNode event = objectMapper.readTree(eventJson);
            return event.get("eventType").asText();
        } catch (Exception e) {
            return "unknown";
        }
    }
    
    private static String createAnomalyAlert(String userId, Long count, TimeWindow window) {
        try {
            return objectMapper.writeValueAsString(Map.of(
                "userId", userId,
                "eventCount", count,
                "windowStart", window.start(),
                "windowEnd", window.end(),
                "alertType", "HIGH_ACTIVITY",
                "timestamp", System.currentTimeMillis()
            ));
        } catch (Exception e) {
            return "{}";
        }
    }
    
    private static String createMetricEvent(String eventType, Long count, TimeWindow window) {
        try {
            return objectMapper.writeValueAsString(Map.of(
                "eventType", eventType,
                "count", count,
                "windowStart", window.start(),
                "windowEnd", window.end(),
                "timestamp", System.currentTimeMillis()
            ));
        } catch (Exception e) {
            return "{}";
        }
    }
    
    // Custom transformer for event validation
    private static class EventValidator implements ValueTransformer<String, String> {
        private KeyValueStore<String, String> userStore;
        
        @Override
        public void init(ProcessorContext context) {
            this.userStore = (KeyValueStore<String, String>) context.getStateStore("user-store");
        }
        
        @Override
        public String transform(String value) {
            try {
                JsonNode event = objectMapper.readTree(value);
                String userId = event.get("userId").asText();
                
                // Enrich with user data if available
                String userData = userStore.get(userId);
                if (userData != null) {
                    JsonNode userProfile = objectMapper.readTree(userData);
                    ((com.fasterxml.jackson.databind.node.ObjectNode) event)
                        .set("userProfile", userProfile);
                }
                
                return objectMapper.writeValueAsString(event);
            } catch (Exception e) {
                return value;
            }
        }
        
        @Override
        public void close() {
            // Cleanup if needed
        }
    }
}
```

### 6.2 Apache Flink Stream Processing

#### Flink DataStream Processing

```java
// FlinkStreamProcessor.java
package com.dataplatform.flink;

import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.functions.ReduceFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction;
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;
import org.apache.flink.util.Collector;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.time.Duration;

public class FlinkStreamProcessor {
    
    private static final ObjectMapper objectMapper = new ObjectMapper();
    
    public static void main(String[] args) throws Exception {
        
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(4);
        env.enableCheckpointing(5000); // Checkpoint every 5 seconds
        
        // Configure Kafka source
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("localhost:9092")
            .setTopics("raw-events")
            .setGroupId("flink-processor")
            .setStartingOffsets(OffsetsInitializer.earliest())
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();
        
        // Create data stream
        DataStream<String> rawEvents = env.fromSource(
            source,
            WatermarkStrategy.<String>forBoundedOutOfOrderness(Duration.ofSeconds(20))
                .withTimestampAssigner((event, timestamp) -> extractTimestamp(event)),
            "kafka-source"
        );
        
        // Process events
        DataStream<EventMetrics> processedEvents = rawEvents
            .filter(FlinkStreamProcessor::isValidEvent)
            .map(new EventParser())
            .keyBy(event -> event.userId)
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            .process(new EventAggregator());
        
        // Detect anomalies
        DataStream<AnomalyAlert> anomalies = processedEvents
            .filter(metrics -> metrics.eventCount > 100)
            .map(new AnomalyDetector());
        
        // Output results
        processedEvents.print("Processed Events");
        anomalies.print("Anomalies");
        
        env.execute("Flink Stream Processing Job");
    }
    
    private static boolean isValidEvent(String eventJson) {
        try {
            JsonNode event = objectMapper.readTree(eventJson);
            return event.has("userId") && 
                   event.has("eventType") && 
                   event.has("timestamp");
        } catch (Exception e) {
            return false;
        }
    }
    
    private static long extractTimestamp(String eventJson) {
        try {
            JsonNode event = objectMapper.readTree(eventJson);
            return event.get("timestamp").asLong();
        } catch (Exception e) {
            return System.currentTimeMillis();
        }
    }
    
    // Event parsing function
    public static class EventParser implements MapFunction<String, Event> {
        @Override
        public Event map(String eventJson) throws Exception {
            JsonNode event = objectMapper.readTree(eventJson);
            return new Event(
                event.get("userId").asText(),
                event.get("eventType").asText(),
                event.get("timestamp").asLong()
            );
        }
    }
    
    // Event aggregation window function
    public static class EventAggregator extends ProcessWindowFunction<Event, EventMetrics, String, TimeWindow> {
        @Override
        public void process(String userId, Context context, Iterable<Event> events, Collector<EventMetrics> out) {
            long count = 0;
            String lastEventType = "";
            
            for (Event event : events) {
                count++;
                lastEventType = event.eventType;
            }
            
            out.collect(new EventMetrics(
                userId,
                count,
                lastEventType,
                context.window().getStart(),
                context.window().getEnd()
            ));
        }
    }
    
    // Anomaly detection function
    public static class AnomalyDetector implements MapFunction<EventMetrics, AnomalyAlert> {
        @Override
        public AnomalyAlert map(EventMetrics metrics) throws Exception {
            return new AnomalyAlert(
                metrics.userId,
                metrics.eventCount,
                "HIGH_ACTIVITY",
                System.currentTimeMillis()
            );
        }
    }
    
    // Data classes
    public static class Event {
        public String userId;
        public String eventType;
        public long timestamp;
        
        public Event(String userId, String eventType, long timestamp) {
            this.userId = userId;
            this.eventType = eventType;
            this.timestamp = timestamp;
        }
    }
    
    public static class EventMetrics {
        public String userId;
        public long eventCount;
        public String lastEventType;
        public long windowStart;
        public long windowEnd;
        
        public EventMetrics(String userId, long eventCount, String lastEventType, long windowStart, long windowEnd) {
            this.userId = userId;
            this.eventCount = eventCount;
            this.lastEventType = lastEventType;
            this.windowStart = windowStart;
            this.windowEnd = windowEnd;
        }
    }
    
    public static class AnomalyAlert {
        public String userId;
        public long eventCount;
        public String alertType;
        public long timestamp;
        
        public AnomalyAlert(String userId, long eventCount, String alertType, long timestamp) {
            this.userId = userId;
            this.eventCount = eventCount;
            this.alertType = alertType;
            this.timestamp = timestamp;
        }
    }
}
```

---

## ü§ñ Machine Learning Pipeline Integration

### 7.1 MLflow Integration Framework

#### ML Pipeline with Spark MLlib

```python
# ml_pipeline_framework.py
import mlflow
import mlflow.spark
from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from typing import Dict, List, Tuple
import logging

class MLPipelineFramework:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
        
        # Initialize MLflow
        mlflow.set_tracking_uri("http://localhost:5000")
        mlflow.set_experiment("data-platform-ml")
    
    def create_feature_pipeline(self, feature_config: Dict) -> Pipeline:
        """Create feature engineering pipeline"""
        
        stages = []
        
        # String indexing for categorical features
        if 'categorical_features' in feature_config:
            for cat_feature in feature_config['categorical_features']:
                indexer = StringIndexer(
                    inputCol=cat_feature,
                    outputCol=f"{cat_feature}_indexed",
                    handleInvalid="keep"
                )
                stages.append(indexer)
        
        # Vector assembly
        feature_cols = feature_config.get('numeric_features', [])
        if 'categorical_features' in feature_config:
            feature_cols.extend([f"{cat}_indexed" for cat in feature_config['categorical_features']])
        
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features_raw"
        )
        stages.append(assembler)
        
        # Feature scaling
        if feature_config.get('scale_features', True):
            scaler = StandardScaler(
                inputCol="features_raw",
                outputCol="features",
                withStd=True,
                withMean=True
            )
            stages.append(scaler)
        
        return Pipeline(stages=stages)
    
    def train_model(self, train_df, feature_config: Dict, model_config: Dict) -> Tuple:
        """Train ML model with hyperparameter tuning"""
        
        with mlflow.start_run():
            
            # Log parameters
            mlflow.log_params(model_config)
            mlflow.log_params(feature_config)
            
            # Create feature pipeline
            feature_pipeline = self.create_feature_pipeline(feature_config)
            
            # Create model
            if model_config['algorithm'] == 'random_forest':
                model = RandomForestClassifier(
                    featuresCol="features",
                    labelCol=model_config['target_column'],
                    numTrees=model_config.get('num_trees', 100),
                    maxDepth=model_config.get('max_depth', 10)
                )
            else:
                raise ValueError(f"Unsupported algorithm: {model_config['algorithm']}")
            
            # Create full pipeline
            full_pipeline = Pipeline(stages=feature_pipeline.getStages() + [model])
            
            # Hyperparameter tuning
            if model_config.get('tune_hyperparameters', False):
                param_grid = ParamGridBuilder() \
                    .addGrid(model.numTrees, [50, 100, 200]) \
                    .addGrid(model.maxDepth, [5, 10, 15]) \
                    .build()
                
                evaluator = BinaryClassificationEvaluator(
                    labelCol=model_config['target_column'],
                    metricName="areaUnderROC"
                )
                
                cv = CrossValidator(
                    estimator=full_pipeline,
                    estimatorParamMaps=param_grid,
                    evaluator=evaluator,
                    numFolds=3
                )
                
                cv_model = cv.fit(train_df)
                best_model = cv_model.bestModel
                
                # Log best parameters
                best_params = {
                    'best_num_trees': best_model.stages[-1].getNumTrees(),
                    'best_max_depth': best_model.stages[-1].getMaxDepth()
                }
                mlflow.log_params(best_params)
                
            else:
                best_model = full_pipeline.fit(train_df)
            
            # Log model
            mlflow.spark.log_model(best_model, "model")
            
            return best_model, feature_pipeline
    
    def evaluate_model(self, model, test_df, model_config: Dict) -> Dict:
        """Evaluate model performance"""
        
        predictions = model.transform(test_df)
        
        evaluator = BinaryClassificationEvaluator(
            labelCol=model_config['target_column'],
            metricName="areaUnderROC"
        )
        
        auc = evaluator.evaluate(predictions)
        
        # Calculate additional metrics
        tp = predictions.filter((predictions.prediction == 1) & (predictions[model_config['target_column']] == 1)).count()
        fp = predictions.filter((predictions.prediction == 1) & (predictions[model_config['target_column']] == 0)).count()
        tn = predictions.filter((predictions.prediction == 0) & (predictions[model_config['target_column']] == 0)).count()
        fn = predictions.filter((predictions.prediction == 0) & (predictions[model_config['target_column']] == 1)).count()
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics = {
            'auc': auc,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'true_positives': tp,
            'false_positives': fp,
            'true_negatives': tn,
            'false_negatives': fn
        }
        
        # Log metrics
        mlflow.log_metrics(metrics)
        
        return metrics
    
    def deploy_model(self, model, model_name: str, stage: str = "Production") -> str:
        """Deploy model to MLflow Model Registry"""
        
        # Register model
        model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
        registered_model = mlflow.register_model(model_uri, model_name)
        
        # Transition to stage
        client = mlflow.tracking.MlflowClient()
        client.transition_model_version_stage(
            name=model_name,
            version=registered_model.version,
            stage=stage
        )
        
        self.logger.info(f"Model {model_name} version {registered_model.version} deployed to {stage}")
        
        return registered_model.version
    
    def batch_inference(self, model_name: str, input_df, output_path: str) -> None:
        """Perform batch inference using registered model"""
        
        # Load model from registry
        model = mlflow.spark.load_model(f"models:/{model_name}/Production")
        
        # Make predictions
        predictions = model.transform(input_df)
        
        # Save results
        predictions.select("*", "prediction", "probability").write \
            .mode("overwrite") \
            .parquet(output_path)
        
        self.logger.info(f"Batch inference completed. Results saved to {output_path}")

# Example usage configuration
feature_config = {
    'numeric_features': ['age', 'income', 'credit_score', 'account_balance'],
    'categorical_features': ['gender', 'occupation', 'region'],
    'scale_features': True
}

model_config = {
    'algorithm': 'random_forest',
    'target_column': 'default_risk',
    'num_trees': 100,
    'max_depth': 10,
    'tune_hyperparameters': True
}
```

### 7.2 Real-time Feature Store Integration

#### Feature Store Framework

```python
# feature_store_framework.py
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from typing import Dict, List, Optional
import redis
import json
import logging
from datetime import datetime, timedelta

class FeatureStore:
    def __init__(self, spark_session: SparkSession, redis_config: Dict):
        self.spark = spark_session
        self.redis_client = redis.Redis(**redis_config)
        self.logger = logging.getLogger(__name__)
    
    def create_feature_group(self, name: str, df: DataFrame, 
                           primary_key: str, event_time_col: str,
                           description: str = "") -> None:
        """Create a new feature group"""
        
        # Validate DataFrame
        if primary_key not in df.columns:
            raise ValueError(f"Primary key {primary_key} not found in DataFrame")
        
        if event_time_col not in df.columns:
            raise ValueError(f"Event time column {event_time_col} not found in DataFrame")
        
        # Add metadata columns
        feature_df = df.withColumn("_feature_group", lit(name)) \
                      .withColumn("_created_at", current_timestamp()) \
                      .withColumn("_version", lit("1.0"))
        
        # Store feature group metadata
        metadata = {
            'name': name,
            'description': description,
            'primary_key': primary_key,
            'event_time_col': event_time_col,
            'schema': feature_df.schema.json(),
            'created_at': datetime.now().isoformat(),
            'record_count': feature_df.count()
        }
        
        self.redis_client.hset(f"feature_group:{name}", mapping=metadata)
        
        # Store features in both batch and online stores
        self._store_batch_features(name, feature_df)
        self._store_online_features(name, feature_df, primary_key)
        
        self.logger.info(f"Created feature group: {name} with {metadata['record_count']} records")
    
    def get_batch_features(self, feature_group: str, 
                          feature_names: List[str] = None,
                          start_time: datetime = None,
                          end_time: datetime = None) -> DataFrame:
        """Retrieve batch features for training"""
        
        # Load from batch store (Delta Lake/Parquet)
        batch_path = f"s3://feature-store/batch/{feature_group}/"
        df = self.spark.read.parquet(batch_path)
        
        # Apply filters
        if start_time:
            df = df.filter(col("_created_at") >= lit(start_time))
        
        if end_time:
            df = df.filter(col("_created_at") <= lit(end_time))
        
        # Select specific features
        if feature_names:
            metadata = self.redis_client.hgetall(f"feature_group:{feature_group}")
            primary_key = metadata[b'primary_key'].decode()
            event_time_col = metadata[b'event_time_col'].decode()
            
            select_cols = [primary_key, event_time_col] + feature_names
            df = df.select(*select_cols)
        
        return df
    
    def get_online_features(self, feature_group: str, 
                           entity_ids: List[str],
                           feature_names: List[str] = None) -> Dict:
        """Retrieve online features for real-time inference"""
        
        features = {}
        
        for entity_id in entity_ids:
            feature_key = f"features:{feature_group}:{entity_id}"
            feature_data = self.redis_client.hgetall(feature_key)
            
            if feature_data:
                entity_features = {
                    k.decode(): json.loads(v.decode()) 
                    for k, v in feature_data.items()
                }
                
                # Filter specific features if requested
                if feature_names:
                    entity_features = {
                        k: v for k, v in entity_features.items() 
                        if k in feature_names
                    }
                
                features[entity_id] = entity_features
        
        return features
    
    def compute_time_series_features(self, df: DataFrame, 
                                   entity_col: str, time_col: str,
                                   value_cols: List[str],
                                   windows: List[str]) -> DataFrame:
        """Compute time-series aggregation features"""
        
        from pyspark.sql.window import Window
        
        # Sort by entity and time
        df_sorted = df.orderBy(entity_col, time_col)
        
        enhanced_df = df_sorted
        
        for window_spec in windows:
            # Parse window specification (e.g., "7d", "30d", "1h")
            if window_spec.endswith('d'):
                days = int(window_spec[:-1])
                window_seconds = days * 24 * 3600
            elif window_spec.endswith('h'):
                hours = int(window_spec[:-1])
                window_seconds = hours * 3600
            else:
                continue
            
            # Create window specification
            window = Window.partitionBy(entity_col) \
                          .orderBy(col(time_col).cast("long")) \
                          .rangeBetween(-window_seconds, 0)
            
            # Compute aggregations for each value column
            for value_col in value_cols:
                enhanced_df = enhanced_df.withColumn(
                    f"{value_col}_avg_{window_spec}",
                    avg(value_col).over(window)
                ).withColumn(
                    f"{value_col}_sum_{window_spec}",
                    sum(value_col).over(window)
                ).withColumn(
                    f"{value_col}_count_{window_spec}",
                    count(value_col).over(window)
                ).withColumn(
                    f"{value_col}_max_{window_spec}",
                    max(value_col).over(window)
                ).withColumn(
                    f"{value_col}_min_{window_spec}",
                    min(value_col).over(window)
                )
        
        return enhanced_df
    
    def _store_batch_features(self, feature_group: str, df: DataFrame) -> None:
        """Store features in batch store"""
        
        batch_path = f"s3://feature-store/batch/{feature_group}/"
        
        df.write \
          .mode("overwrite") \
          .option("mergeSchema", "true") \
          .parquet(batch_path)
    
    def _store_online_features(self, feature_group: str, df: DataFrame, 
                             primary_key: str) -> None:
        """Store features in online store (Redis)"""
        
        # Convert to key-value format for Redis
        feature_rows = df.collect()
        
        pipe = self.redis_client.pipeline()
        
        for row in feature_rows:
            entity_id = row[primary_key]
            feature_key = f"features:{feature_group}:{entity_id}"
            
            # Convert row to dictionary, excluding metadata
            feature_dict = {
                col: row[col] for col in df.columns 
                if not col.startswith('_') and col != primary_key
            }
            
            # Store as hash in Redis
            for feature_name, feature_value in feature_dict.items():
                pipe.hset(feature_key, feature_name, json.dumps(feature_value))
            
            # Set TTL for online features (e.g., 7 days)
            pipe.expire(feature_key, 7 * 24 * 3600)
        
        pipe.execute()
        
        self.logger.info(f"Stored {len(feature_rows)} feature records in online store")
    
    def create_feature_view(self, name: str, feature_groups: List[str],
                           join_keys: Dict[str, str],
                           features: Dict[str, List[str]]) -> None:
        """Create a feature view that joins multiple feature groups"""
        
        # Load feature groups
        dfs = {}
        for fg_name in feature_groups:
            dfs[fg_name] = self.get_batch_features(fg_name, features.get(fg_name))
        
        # Join feature groups
        result_df = None
        for i, (fg_name, df) in enumerate(dfs.items()):
            if i == 0:
                result_df = df
            else:
                join_key = join_keys.get(fg_name, 'id')
                result_df = result_df.join(df, join_key, 'left')
        
        # Store feature view
        view_path = f"s3://feature-store/views/{name}/"
        result_df.write.mode("overwrite").parquet(view_path)
        
        # Store metadata
        view_metadata = {
            'name': name,
            'feature_groups': feature_groups,
            'join_keys': json.dumps(join_keys),
            'features': json.dumps(features),
            'created_at': datetime.now().isoformat()
        }
        
        self.redis_client.hset(f"feature_view:{name}", mapping=view_metadata)
        
        self.logger.info(f"Created feature view: {name}")

# Example usage
redis_config = {
    'host': 'localhost',
    'port': 6379,
    'db': 0
}

# Create feature store
feature_store = FeatureStore(spark, redis_config)

# Create customer features
customer_features = spark.sql("""
    SELECT 
        customer_id,
        age,
        income,
        credit_score,
        account_balance,
        registration_date as event_time
    FROM customers
""")

feature_store.create_feature_group(
    name="customer_demographics",
    df=customer_features,
    primary_key="customer_id",
    event_time_col="event_time",
    description="Customer demographic and financial features"
)
```

---

## ‚ö° Advanced Performance Optimization

### 8.1 Distributed Computing Optimization

#### Spark Performance Tuning Framework

```python
# spark_performance_tuning.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from typing import Dict, List, Optional
import logging
import time

class SparkPerformanceTuner:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
    
    def analyze_data_skew(self, df, partition_cols: List[str]) -> Dict:
        """Analyze data skew in partitions"""
        
        skew_analysis = {}
        
        for col in partition_cols:
            # Count records per partition value
            partition_counts = df.groupBy(col).count().collect()
            
            counts = [row['count'] for row in partition_counts]
            if counts:
                avg_count = sum(counts) / len(counts)
                max_count = max(counts)
                min_count = min(counts)
                
                skew_ratio = max_count / avg_count if avg_count > 0 else 0
                
                skew_analysis[col] = {
                    'partition_count': len(counts),
                    'avg_records_per_partition': avg_count,
                    'max_records_per_partition': max_count,
                    'min_records_per_partition': min_count,
                    'skew_ratio': skew_ratio,
                    'is_skewed': skew_ratio > 3.0  # Threshold for skew detection
                }
        
        return skew_analysis
    
    def optimize_join_strategy(self, left_df, right_df, join_condition: str,
                             join_type: str = "inner") -> Dict:
        """Optimize join strategy based on data characteristics"""
        
        left_count = left_df.count()
        right_count = right_df.count()
        
        # Estimate data sizes (approximate)
        left_size_mb = left_count * len(left_df.columns) * 8 / (1024 * 1024)  # Rough estimate
        right_size_mb = right_count * len(right_df.columns) * 8 / (1024 * 1024)
        
        optimization_strategy = {
            'left_count': left_count,
            'right_count': right_count,
            'left_size_mb': left_size_mb,
            'right_size_mb': right_size_mb,
            'recommended_strategy': 'sort_merge_join'  # Default
        }
        
        # Broadcast join optimization
        broadcast_threshold = 200  # MB
        if right_size_mb < broadcast_threshold:
            optimization_strategy['recommended_strategy'] = 'broadcast_join'
            optimization_strategy['broadcast_side'] = 'right'
        elif left_size_mb < broadcast_threshold:
            optimization_strategy['recommended_strategy'] = 'broadcast_join'
            optimization_strategy['broadcast_side'] = 'left'
        
        # Bucket join optimization
        if left_count > 1000000 and right_count > 1000000:
            optimization_strategy['consider_bucketing'] = True
            optimization_strategy['recommended_buckets'] = min(200, max(left_count, right_count) // 50000)
        
        return optimization_strategy
    
    def implement_join_optimization(self, left_df, right_df, join_condition: str,
                                  optimization_strategy: Dict, join_type: str = "inner"):
        """Implement optimized join based on strategy"""
        
        strategy = optimization_strategy['recommended_strategy']
        
        if strategy == 'broadcast_join':
            broadcast_side = optimization_strategy['broadcast_side']
            
            if broadcast_side == 'right':
                from pyspark.sql.functions import broadcast
                return left_df.join(broadcast(right_df), join_condition, join_type)
            else:
                from pyspark.sql.functions import broadcast
                return broadcast(left_df).join(right_df, join_condition, join_type)
        
        elif strategy == 'sort_merge_join':
            # Ensure both DataFrames are properly partitioned
            left_partitioned = left_df.repartition(200)  # Adjust based on cluster size
            right_partitioned = right_df.repartition(200)
            
            return left_partitioned.join(right_partitioned, join_condition, join_type)
        
        else:
            # Default join
            return left_df.join(right_df, join_condition, join_type)
    
    def optimize_aggregations(self, df, group_cols: List[str], 
                            agg_exprs: List[str]) -> Dict:
        """Optimize aggregation operations"""
        
        # Analyze cardinality of grouping columns
        cardinality_analysis = {}
        for col in group_cols:
            distinct_count = df.select(col).distinct().count()
            total_count = df.count()
            cardinality_ratio = distinct_count / total_count if total_count > 0 else 0
            
            cardinality_analysis[col] = {
                'distinct_count': distinct_count,
                'cardinality_ratio': cardinality_ratio
            }
        
        # Determine optimization strategy
        total_distinct_combinations = 1
        for col_stats in cardinality_analysis.values():
            total_distinct_combinations *= col_stats['distinct_count']
        
        optimization_strategy = {
            'cardinality_analysis': cardinality_analysis,
            'total_combinations': total_distinct_combinations,
            'recommended_partitions': min(400, max(50, total_distinct_combinations // 1000))
        }
        
        # High cardinality optimization
        if total_distinct_combinations > 1000000:
            optimization_strategy['use_two_phase_aggregation'] = True
            optimization_strategy['pre_aggregate_partitions'] = 1000
        
        return optimization_strategy
    
    def implement_aggregation_optimization(self, df, group_cols: List[str],
                                         agg_exprs: List[str],
                                         optimization_strategy: Dict):
        """Implement optimized aggregation"""
        
        if optimization_strategy.get('use_two_phase_aggregation', False):
            # Two-phase aggregation for high cardinality
            pre_agg_partitions = optimization_strategy['pre_aggregate_partitions']
            
            # Phase 1: Pre-aggregate with more partitions
            pre_aggregated = df.repartition(pre_agg_partitions) \
                              .groupBy(*group_cols) \
                              .agg(*[expr(agg_expr) for agg_expr in agg_exprs])
            
            # Phase 2: Final aggregation with fewer partitions
            final_partitions = optimization_strategy['recommended_partitions']
            result = pre_aggregated.coalesce(final_partitions) \
                                  .groupBy(*group_cols) \
                                  .agg(*[expr(agg_expr) for agg_expr in agg_exprs])
        else:
            # Standard aggregation with optimized partitioning
            partitions = optimization_strategy['recommended_partitions']
            result = df.repartition(partitions, *group_cols) \
                      .groupBy(*group_cols) \
                      .agg(*[expr(agg_expr) for agg_expr in agg_exprs])
        
        return result
    
    def monitor_query_execution(self, df, query_name: str) -> Dict:
        """Monitor and analyze query execution"""
        
        start_time = time.time()
        
        # Force execution and collect metrics
        result_count = df.count()
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Get execution plan
        execution_plan = df.explain(extended=True)
        
        # Analyze partitions
        num_partitions = df.rdd.getNumPartitions()
        
        metrics = {
            'query_name': query_name,
            'execution_time_seconds': execution_time,
            'result_count': result_count,
            'num_partitions': num_partitions,
            'records_per_second': result_count / execution_time if execution_time > 0 else 0,
            'execution_plan': execution_plan
        }
        
        self.logger.info(f"Query Execution Metrics for {query_name}:")
        self.logger.info(f"  - Execution Time: {execution_time:.2f} seconds")
        self.logger.info(f"  - Result Count: {result_count:,}")
        self.logger.info(f"  - Partitions: {num_partitions}")
        self.logger.info(f"  - Records/Second: {metrics['records_per_second']:,.0f}")
        
        return metrics

# Example usage
tuner = SparkPerformanceTuner(spark)

# Analyze data skew
skew_analysis = tuner.analyze_data_skew(df, ['customer_segment', 'region'])

# Optimize join
join_strategy = tuner.optimize_join_strategy(customers_df, orders_df, "customer_id")
optimized_join = tuner.implement_join_optimization(
    customers_df, orders_df, "customer_id", join_strategy
)

# Monitor execution
metrics = tuner.monitor_query_execution(optimized_join, "customer_orders_join")
```

### 8.2 Memory and Resource Optimization

#### Resource Management Framework

```python
# resource_optimization.py
import psutil
import gc
from pyspark.sql import SparkSession
from typing import Dict, List, Optional
import logging
import json

class ResourceOptimizer:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
    
    def analyze_memory_usage(self) -> Dict:
        """Analyze current memory usage"""
        
        # System memory
        system_memory = psutil.virtual_memory()
        
        # Spark memory (from Spark UI metrics if available)
        spark_context = self.spark.sparkContext
        
        memory_analysis = {
            'system_memory': {
                'total_gb': system_memory.total / (1024**3),
                'available_gb': system_memory.available / (1024**3),
                'used_gb': system_memory.used / (1024**3),
                'percent_used': system_memory.percent
            },
            'spark_memory': {
                'executor_memory': spark_context.getConf().get('spark.executor.memory', 'Not set'),
                'driver_memory': spark_context.getConf().get('spark.driver.memory', 'Not set'),
                'executor_cores': spark_context.getConf().get('spark.executor.cores', 'Not set'),
                'max_result_size': spark_context.getConf().get('spark.driver.maxResultSize', 'Not set')
            }
        }
        
        return memory_analysis
    
    def optimize_spark_configuration(self, workload_characteristics: Dict) -> Dict:
        """Generate optimized Spark configuration"""
        
        data_size_gb = workload_characteristics.get('data_size_gb', 10)
        num_executors = workload_characteristics.get('num_executors', 4)
        cores_per_executor = workload_characteristics.get('cores_per_executor', 4)
        
        # Calculate memory requirements
        executor_memory_gb = max(4, min(16, data_size_gb / num_executors * 1.5))
        driver_memory_gb = max(2, min(8, data_size_gb * 0.1))
        
        optimized_config = {
            # Memory settings
            'spark.executor.memory': f'{int(executor_memory_gb)}g',
            'spark.driver.memory': f'{int(driver_memory_gb)}g',
            'spark.executor.memoryFraction': '0.8',
            'spark.storage.memoryFraction': '0.5',
            
            # Core settings
            'spark.executor.cores': str(cores_per_executor),
            'spark.executor.instances': str(num_executors),
            
            # Shuffle optimization
            'spark.sql.shuffle.partitions': str(num_executors * cores_per_executor * 4),
            'spark.shuffle.compress': 'true',
            'spark.shuffle.spill.compress': 'true',
            
            # Serialization
            'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',
            'spark.kryo.unsafe': 'true',
            
            # Adaptive query execution
            'spark.sql.adaptive.enabled': 'true',
            'spark.sql.adaptive.coalescePartitions.enabled': 'true',
            'spark.sql.adaptive.skewJoin.enabled': 'true',
            
            # Dynamic allocation
            'spark.dynamicAllocation.enabled': 'true',
            'spark.dynamicAllocation.minExecutors': '1',
            'spark.dynamicAllocation.maxExecutors': str(num_executors * 2),
            'spark.dynamicAllocation.initialExecutors': str(num_executors),
        }
        
        return optimized_config
    
    def implement_caching_strategy(self, dataframes: Dict[str, any], 
                                 access_patterns: Dict[str, str]) -> Dict:
        """Implement intelligent caching strategy"""
        
        caching_decisions = {}
        
        for df_name, df in dataframes.items():
            access_pattern = access_patterns.get(df_name, 'single_use')
            
            # Estimate DataFrame size
            estimated_size_mb = df.count() * len(df.columns) * 8 / (1024 * 1024)
            
            cache_decision = {
                'should_cache': False,
                'storage_level': 'MEMORY_AND_DISK',
                'estimated_size_mb': estimated_size_mb,
                'reason': ''
            }
            
            if access_pattern == 'multiple_use' and estimated_size_mb < 1000:
                cache_decision['should_cache'] = True
                cache_decision['storage_level'] = 'MEMORY_ONLY'
                cache_decision['reason'] = 'Multiple access, fits in memory'
                
            elif access_pattern == 'multiple_use' and estimated_size_mb < 5000:
                cache_decision['should_cache'] = True
                cache_decision['storage_level'] = 'MEMORY_AND_DISK'
                cache_decision['reason'] = 'Multiple access, large dataset'
                
            elif access_pattern == 'iterative' and estimated_size_mb < 2000:
                cache_decision['should_cache'] = True
                cache_decision['storage_level'] = 'MEMORY_AND_DISK_SER'
                cache_decision['reason'] = 'Iterative access, serialized for efficiency'
            
            # Apply caching decision
            if cache_decision['should_cache']:
                storage_level = getattr(
                    self.spark.sparkContext._jvm.org.apache.spark.storage.StorageLevel,
                    cache_decision['storage_level']
                )()
                df.persist(storage_level)
                
                self.logger.info(f"Cached {df_name} with {cache_decision['storage_level']}")
            
            caching_decisions[df_name] = cache_decision
        
        return caching_decisions
    
    def cleanup_resources(self, dataframes_to_unpersist: List[str] = None) -> None:
        """Clean up cached resources"""
        
        # Unpersist specific DataFrames if provided
        if dataframes_to_unpersist:
            for df_name in dataframes_to_unpersist:
                # This would need to be implemented with actual DataFrame references
                self.logger.info(f"Unpersisted {df_name}")
        
        # Clear Spark catalog cache
        self.spark.catalog.clearCache()
        
        # Force garbage collection
        gc.collect()
        
        # Log memory status after cleanup
        memory_status = self.analyze_memory_usage()
        self.logger.info(f"Memory after cleanup: {memory_status['system_memory']['percent_used']:.1f}% used")
    
    def monitor_resource_utilization(self) -> Dict:
        """Monitor real-time resource utilization"""
        
        # CPU utilization
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()
        
        # Memory utilization
        memory = psutil.virtual_memory()
        
        # Disk I/O
        disk_io = psutil.disk_io_counters()
        
        # Network I/O
        network_io = psutil.net_io_counters()
        
        utilization_metrics = {
            'timestamp': time.time(),
            'cpu': {
                'percent_used': cpu_percent,
                'core_count': cpu_count
            },
            'memory': {
                'percent_used': memory.percent,
                'available_gb': memory.available / (1024**3)
            },
            'disk_io': {
                'read_mb_per_sec': disk_io.read_bytes / (1024**2),
                'write_mb_per_sec': disk_io.write_bytes / (1024**2)
            },
            'network_io': {
                'bytes_sent_mb': network_io.bytes_sent / (1024**2),
                'bytes_recv_mb': network_io.bytes_recv / (1024**2)
            }
        }
        
        return utilization_metrics

# Example usage
optimizer = ResourceOptimizer(spark)

# Analyze current memory usage
memory_analysis = optimizer.analyze_memory_usage()

# Optimize configuration for workload
workload_chars = {
    'data_size_gb': 50,
    'num_executors': 8,
    'cores_per_executor': 4
}

optimized_config = optimizer.optimize_spark_configuration(workload_chars)

# Implement caching strategy
dataframes = {
    'customers': customers_df,
    'orders': orders_df,
    'products': products_df
}

access_patterns = {
    'customers': 'multiple_use',
    'orders': 'single_use',
    'products': 'iterative'
}

caching_strategy = optimizer.implement_caching_strategy(dataframes, access_patterns)
```

---

## üìö Real-World Implementation Scenarios

### 9.1 Real-time Fraud Detection System

**Scenario:** Financial services company needs real-time fraud detection

**Requirements:**
- Process 100K+ transactions per second
- Sub-100ms detection latency
- 99.9% accuracy with minimal false positives
- Integration with existing systems

**Complete Solution:**

```yaml
# fraud_detection_architecture.yaml
fraud_detection_system:
  name: "real_time_fraud_detection"
  sla_requirements:
    latency: "< 100ms"
    throughput: "100k_tps"
    accuracy: "> 99.9%"
    availability: "99.99%"
  
  stream_processing:
    technology: "apache_flink"
    parallelism: 64
    checkpointing: "5_seconds"
    state_backend: "rocksdb"
    
  feature_engineering:
    real_time_features:
      - "transaction_amount"
      - "merchant_category"
      - "time_since_last_transaction"
      - "location_velocity"
      - "spending_pattern_deviation"
    
    aggregation_windows:
      - "1_minute"
      - "5_minutes" 
      - "1_hour"
      - "24_hours"
    
  ml_models:
    primary_model:
      algorithm: "gradient_boosting"
      features: 45
      training_frequency: "daily"
      
    ensemble_models:
      - "random_forest"
      - "neural_network"
      - "isolation_forest"
    
  decision_engine:
    rules_engine: "drools"
    ml_threshold: 0.85
    rule_overrides: "enabled"
    
  integration:
    input_streams:
      - "transaction_events"
      - "customer_profiles"
      - "merchant_data"
    
    output_streams:
      - "fraud_alerts"
      - "approved_transactions"
      - "model_feedback"
```

### 9.2 IoT Data Processing Pipeline

**Implementation Example:**

```python
# iot_processing_pipeline.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.streaming import StreamingQuery
import logging

class IoTProcessingPipeline:
    def __init__(self, spark_session: SparkSession):
        self.spark = spark_session
        self.logger = logging.getLogger(__name__)
    
    def create_sensor_data_schema(self) -> StructType:
        """Define schema for IoT sensor data"""
        return StructType([
            StructField("device_id", StringType(), False),
            StructField("sensor_type", StringType(), False),
            StructField("timestamp", TimestampType(), False),
            StructField("value", DoubleType(), False),
            StructField("unit", StringType(), True),
            StructField("location", StructType([
                StructField("latitude", DoubleType(), True),
                StructField("longitude", DoubleType(), True)
            ]), True),
            StructField("metadata", MapType(StringType(), StringType()), True)
        ])
    
    def process_sensor_stream(self, kafka_config: Dict) -> StreamingQuery:
        """Process real-time sensor data stream"""
        
        schema = self.create_sensor_data_schema()
        
        # Read from Kafka
        sensor_stream = self.spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_config['bootstrap_servers']) \
            .option("subscribe", kafka_config['topic']) \
            .option("startingOffsets", "latest") \
            .load()
        
        # Parse JSON data
        parsed_stream = sensor_stream.select(
            from_json(col("value").cast("string"), schema).alias("data"),
            col("timestamp").alias("kafka_timestamp")
        ).select("data.*", "kafka_timestamp")
        
        # Data quality checks
        validated_stream = parsed_stream.filter(
            col("device_id").isNotNull() &
            col("sensor_type").isNotNull() &
            col("value").isNotNull() &
            (col("value") >= -1000) &  # Reasonable sensor value range
            (col("value") <= 1000)
        )
        
        # Enrich with device metadata
        enriched_stream = validated_stream.withColumn(
            "processing_time", current_timestamp()
        ).withColumn(
            "hour_of_day", hour(col("timestamp"))
        ).withColumn(
            "day_of_week", dayofweek(col("timestamp"))
        )
        
        # Anomaly detection using statistical methods
        anomaly_stream = enriched_stream.withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window(col("timestamp"), "5 minutes"),
                col("device_id"),
                col("sensor_type")
            ).agg(
                avg("value").alias("avg_value"),
                stddev("value").alias("stddev_value"),
                count("*").alias("reading_count"),
                max("value").alias("max_value"),
                min("value").alias("min_value")
            ).withColumn(
                "is_anomaly",
                when(
                    (col("stddev_value") > 50) |  # High variance
                    (col("reading_count") < 5) |   # Too few readings
                    (abs(col("max_value") - col("min_value")) > 100), # Large range
                    True
                ).otherwise(False)
            )
        
        # Write results to multiple sinks
        query = enriched_stream.writeStream \
            .outputMode("append") \
            .format("console") \
            .option("truncate", False) \
            .trigger(processingTime='30 seconds') \
            .start()
        
        return query
    
    def create_device_health_dashboard(self, processed_data):
        """Create real-time device health metrics"""
        
        device_health = processed_data.withWatermark("timestamp", "15 minutes") \
            .groupBy(
                window(col("timestamp"), "10 minutes", "5 minutes"),
                col("device_id")
            ).agg(
                count("*").alias("total_readings"),
                avg("value").alias("avg_reading"),
                last("timestamp").alias("last_seen"),
                countDistinct("sensor_type").alias("active_sensors")
            ).withColumn(
                "health_score",
                when(col("total_readings") >= 100, 1.0)
                .when(col("total_readings") >= 50, 0.8)
                .when(col("total_readings") >= 10, 0.6)
                .otherwise(0.3)
            ).withColumn(
                "status",
                when(col("health_score") >= 0.8, "healthy")
                .when(col("health_score") >= 0.6, "warning")
                .otherwise("critical")
            )
        
        return device_health

# Example usage
iot_pipeline = IoTProcessingPipeline(spark)

kafka_config = {
    'bootstrap_servers': 'localhost:9092',
    'topic': 'sensor-data'
}

# Start processing
query = iot_pipeline.process_sensor_stream(kafka_config)

# Wait for termination
query.awaitTermination()
```

---

## üéØ Implementation Checklist

### Advanced Processing Pipeline Checklist

**Phase 1: Stream Processing Setup**
- [ ] Kafka/Pulsar cluster configuration
- [ ] Stream processing framework selection (Flink/Kafka Streams)
- [ ] Exactly-once processing guarantees
- [ ] State management and checkpointing
- [ ] Watermark and late data handling

**Phase 2: ML Pipeline Integration**
- [ ] Feature store implementation
- [ ] Model training and validation pipelines
- [ ] Model registry and versioning
- [ ] A/B testing framework for models
- [ ] Real-time and batch inference systems

**Phase 3: Performance Optimization**
- [ ] Resource allocation and auto-scaling
- [ ] Memory and CPU optimization
- [ ] Data partitioning and bucketing strategies
- [ ] Caching and materialization policies
- [ ] Query optimization and indexing

**Phase 4: Monitoring & Observability**
- [ ] Real-time metrics and alerting
- [ ] Distributed tracing implementation
- [ ] Performance profiling and bottleneck analysis
- [ ] Data quality monitoring
- [ ] Cost optimization tracking

---

## üìä Success Metrics

### Advanced Processing KPIs

**Stream Processing Performance:**
- End-to-end latency: < 100ms for 95th percentile
- Throughput: Target events per second achieved
- Exactly-once processing: 100% guarantee
- Fault tolerance: < 30 seconds recovery time

**ML Pipeline Efficiency:**
- Model training time: Baseline vs. optimized
- Feature computation latency: < 10ms for online features
- Model accuracy: Maintained or improved over time
- Deployment frequency: Daily model updates capability

**Resource Optimization:**
- CPU utilization: 70-85% optimal range
- Memory efficiency: < 10% waste
- Cost per processed record: Year-over-year reduction
- Auto-scaling effectiveness: Response time < 2 minutes

---

## üîó Integration Points

### Related Documents
- [Data Storage (05)](05_data_storage.md) - Storage optimization for processing
- [Analytics & Consumption (07)](07_analytics_consumption.md) - Processed data consumption
- [Testing & Quality (08)](08_testing_quality.md) - Processing pipeline testing
- [Monitoring & Support (10)](10_monitoring_support.md) - Operational monitoring
- [Data Processing Part 1](06_data_processing.md) - Core processing concepts

### External Dependencies
- Stream processing platforms (Kafka, Pulsar, Kinesis)
- ML platforms (MLflow, Kubeflow, SageMaker)
- Container orchestration (Kubernetes, Docker Swarm)
- Monitoring systems (Prometheus, Grafana, DataDog)
- Feature stores (Feast, Tecton, AWS Feature Store)

---

**Next Steps:** Proceed to [Analytics & Consumption (07)](07_analytics_consumption.md) for data consumption patterns, or return to [Data Processing Part 1](06_data_processing.md) for core processing concepts.
